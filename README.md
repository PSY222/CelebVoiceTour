# Tour AI Hakathon _ CelebVoiceTour â­ğŸ”Š
## [Service Overview](https://youtu.be/d-fdLNISIXE)
í•œêµ­ê´€ê´‘ê³µì‚¬ ì˜¤ë””ì˜¤ í•´ì„¤ APIì— ìŒì„±ë³€í™˜AIë¥¼ ì ìš©í•´, í•œêµ­ì„ ë°©ë¬¸í•˜ëŠ” ì™¸êµ­ì¸ ê´€ê´‘ê°ì—ê²Œ íŠ¹ë³„í•œ ê´€ê´‘ê²½í—˜ì„ ì„ ì‚¬í•˜ëŠ” 'ë‚´ê·€ì— ì…€ëŸ½'ì…ë‹ˆë‹¤. 'ë‚´ê·€ì— ì…€ëŸ½'ì€ ìµœì•  K-POP ì•„ì´ëŒì˜ ëª©ì†Œë¦¬ë¡œ ì˜¤ë””ì˜¤ í•´ì„¤ì„ ë“£ê³ , í•´ë‹¹ ê°€ìˆ˜ì˜ ìŒì›ì„ í•¨ê»˜ ë“¤ì„ ìˆ˜ ìˆëŠ” í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ì—¬ ì—¬í–‰ì— íŠ¹ë³„í•¨ì„ ë”í•˜ëŠ” ì›¹ ì„œë¹„ìŠ¤ ì…ë‹ˆë‹¤.  <br>

Bringing a unique tourism experience to foreign visitors exploring Korea, the 'CelebVoiceTour' utilizes voice conversion AI in the Korea Tourism Organization's audio guide API. With 'CelebVoiceTour', tourists can enjoy audio guides narrated by their favorite K-POP idols, accompanied by playlists featuring the artist's music, adding an extra layer of specialness to their journey.

## ğŸ”¨ Tech Stack
<img src="https://img.shields.io/badge/Python-3766AB?style=flat-square&logo=Python&logoColor=white"/></a>
<img src="https://img.shields.io/badge/Java-F7DF1E?style=flat-square&logo=JavaScript&logoColor=white"/></a>
<img src="https://img.shields.io/badge/JavaScript-F7DF1E?style=flat-square&logo=JavaScript&logoColor=white"/></a>
<img src="https://img.shields.io/badge/SQL-A4373A?style=flat-square&logo=Microsoft Access&logoColor=white"/></a>
<img src="https://img.shields.io/badge/HTML-E34F26?style=flat-square&logo=HTML5&logoColor=white"/></a>
<img src="https://img.shields.io/badge/CSS-1572B6?style=flat-square&logo=CSS3&logoColor=white"/></a>
<img src="https://img.shields.io/badge/Pytorch-EE4C2C?style=flat-square&logo=TensorFlow&logoColor=white"/></a>

- IDE: IntelliJ IDEA
- Automation tool: Gradle
- DB System: MySQL
- Web Framework : Spring Boot

## ğŸ“ŒAI Tech
Bark Model : Transformer based text-to-audio model by [Suno AI](https://github.com/suno-ai/bark) <br>
The Bark model consists of three transformer models designed to convert text into audio, with distinct stages of processing. It begins by transforming text into semantic tokens using the BERT tokenizer from Hugging Face. These semantic tokens encode the audio content to be generated. In the subsequent step, the model converts semantic tokens into coarse tokens using the EnCodec Codec's first two codebooks from Facebook. Finally, the process involves transforming the first two codebooks from EnCodec into 8 codebooks, providing finer audio details. ([Hugginface model details](https://huggingface.co/suno/bark))

This service utilized voice cloning based on the incredible work from [Serp-Ai](https://huggingface.co/suno/bark). K-POP celebrity's voice was extracted from YouTube video and adjusted noise using Audacity. I transformed K-POP Celebrity's audio file to semantic token using HuBERT model and tokenizer. Then, I went through numerous experiments of generating custom voice by processing pre-generated voice with target celebrity's npz token.

I shared ['VITS_model_with Whisper tutorial'](https://github.com/PSY222/CelebVoiceTour/blob/main/Tutorial_3_VITS_model_with_Whisper) for anyone who wants to try customized voice cloning using VITS model with Whisper.

Refer to this [article](https://www.linkedin.com/pulse/ai-voice-cloning-bark-hubert-practical-guide-felix-leber%3FtrackingId=gMIlVvyXRR2OfBRzSRU1fg%253D%253D/?trackingId=gMIlVvyXRR2OfBRzSRU1fg%3D%3D) for more step-by-step approach.

