{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","mount_file_id":"1cT21GIhG27i43QUw-R8o07aRO9uJoAD_","authorship_tag":"ABX9TyMpcPc0+0AfriZ88P4dDuIf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Train your TTS (VITS model) with Coqui TTS üê∏and Whisper"],"metadata":{"id":"KV2uregnjZMZ"}},{"cell_type":"markdown","source":["This notebook demonstrates a gentle guide to training and testing your own VITS model with Coqui TTS. I have reorganized the Colab notebook code from the amazing YouTuber [NanoNomad](https://www.youtube.com/watch?v=6QAGk_rHipE&t=318s&ab_channel=NanoNomad). I am deeply thankful to all Coqui TTS contributors and the OpenAI team for making these remarkable AI models accessible to all of us."],"metadata":{"id":"Oq-bV4uO88T3"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount(\"/content/drive\")"],"metadata":{"id":"ZzfkU7su8oAk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üí° Define the variables"],"metadata":{"id":"X4mUkkSW_Fo8"}},{"cell_type":"markdown","source":["Additional Information about the directory paths\n","\n","\n","*   ds_name: Dataset name which will be the name of the directory.\n","*   output_dir : All of subfolders created during the training such as config.json or pth file will be located.\n","*   MODEL_FILE: (No need to change) default path of VITS from Coqui.\n","*   RUN_NAME: (Optional) if you want to name your training.\n","- test_path : Save your output audio file for to check your training results.\n","- wavs: Original wavs files chunked for the training.\n","- mono : Converted 22050hz mono wav files will be saved here.\n","- output_path: full path of output_dir where training data will be stored.\n","- open_path: your metadata.csv file will be saved here.\n","\n","\n","\n"],"metadata":{"id":"8n38FRHGlQ8Z"}},{"cell_type":"code","source":["ds_name = \"vits-ds-a\" #@param [\"vits-ds-a\",\"vits-ds-b\",\"vits-ds-c\"]\n","output_dir = \"traineroutput-a\" #@param [\"traineroutput-a\",\"traineroutput-b\",\"traineroutputt-c\"]\n","MODEL_FILE = \"/root/.local/share/tts/tts_models--en--ljspeech--vits/model_file.pth\" #@param {type:\"string\"}\n","RUN_NAME = \"VITS-eng\" #@param {type:\"string\"}\n","\n","test_path = \"/content/drive/MyDrive/\"+ds_name+\"/testoutput/\"\n","wavs = \"/content/drive/MyDrive/\"+ds_name+\"/wavs/\"  #@param {type:\"string\"}\n","mono =  \"/content/drive/MyDrive/\"+ds_name+\"/mono\" #@param {type:\"string\"}\n","\n","output_path = \"/content/drive/MyDrive/\"+ds_name + \"/\"+output_dir+\"/\" #output for the training\n","meta_name = ds_name + '_metadata.csv'\n","open_path = '/content/drive/MyDrive/'+ds_name+'/'+meta_name #metadata.csv path\n"],"metadata":{"id":"wKsMF5Le5DoE","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir /content/drive/MyDrive/$ds_name\n","!mkdir $test_path\n","!mkdir $wavs\n","!mkdir $output_path\n","!mkdir $mono\n","!mkdir $mono/wavs/"],"metadata":{"id":"SjGf8Zrf2L9B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### üìÇUpload your audio files (Optional)\n","\n","*   Recommended for the small number of audio files.\n","*   If there are large number of files, upload directly to $wavs directory."],"metadata":{"id":"SXX5_arqizgw"}},{"cell_type":"code","source":["from google.colab import files\n","print(\"Select your audio samples for the training\")\n","target_files = files.upload()\n","target_files = list(target_files.keys())\n","ds_path = \"/content/drive/MyDrive/\"+ds_name\n","\n","cnt = 0\n","for sample in target_files:\n","    cnt += 1\n","    save_path = os.path.join(ds_path+'/wavs', sample)\n","    !ffmpeg-normalize $sample -nt rms -t=-27 -o {save_path} -ar 16000 -f\n","\n","saved_files= os.listdir(ds_path)\n","print(\"Saved sample files: \" )\n","print(saved_files)\n","\n","assert len(saved_files) == cnt, \"Failed to save audio files\"\n","print(\"Audio files successfully saved\")"],"metadata":{"id":"-mcVFlFM8c7m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Convert audio files to 22050hz mono wav files."],"metadata":{"id":"EJyvXR__KL8_"}},{"cell_type":"code","source":["import os\n","import subprocess\n","\n","def convert_to_mono_22050hz_sox(input_file, output_file):\n","    subprocess.run([\"sox\", input_file, \"-r\", \"22050\", \"-c\", \"1\", output_file])\n","\n","def convert_files_in_directory(ds_dir):\n","    for root, _, files in os.walk(ds_dir):\n","        for file in files:\n","            if file.lower().endswith(('.mp3', '.wav')):\n","                input_file = os.path.join(root, file)\n","                output_file = os.path.join(mono+\"/wavs/\", f\"{os.path.splitext(file)[0]}.wav\")\n","                print(output_file)\n","                convert_to_mono_22050hz_sox(input_file, output_file)\n","        print(\"Files saved successfully to $mono/wavs/\")\n","\n","convert_files_in_directory(wavs)"],"metadata":{"id":"zN7Uj0SPKWG-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Install Whisper and Coqui-ai  üê∏"],"metadata":{"id":"J75l8olM69d6"}},{"cell_type":"markdown","source":["Install whisper"],"metadata":{"id":"nBI-H6L2rHfT"}},{"cell_type":"code","source":["%cd /content\n","!sudo apt install sox\n","!git clone https://github.com/openai/whisper.git\n","!pip install git+https://github.com/openai/whisper.git"],"metadata":{"id":"nFinLwPq62CN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Install Coqui TTS"],"metadata":{"id":"xNxS2LkDjDjZ"}},{"cell_type":"code","source":["%cd /content\n","!sudo apt-get install espeak-ng\n","!git clone https://github.com/coqui-ai/TTS.git\n","!pip install TTS\n","!pip install Trainer==0.0.20"],"metadata":{"id":"oj5u1Vda7RfF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Let's make metadata.csv file ‚úç"],"metadata":{"id":"5P4-Ws0K_Tyq"}},{"cell_type":"code","source":["import glob\n","import pandas as pd\n","from tqdm import tqdm\n","from pathlib import Path\n","\n","all_filenames = []\n","transcript_text = []\n","\n","paths = glob.glob(os.path.join(mono+'/wavs/', '*.wav'))\n","print(\"Number of wav files: \",len(paths))"],"metadata":{"id":"fVKe3OmDCbA5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import whisper\n","model = whisper.load_model(\"medium.en\") #suitable for English audio file\n","# model = whisper.load_model(\"large-v2\") #you may look for other whisper models"],"metadata":{"id":"8j1GR7M2sGum"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(open_path, 'w', encoding='utf-8') as outfile:\n","\tfor filepath in paths:\n","\t\tbase = os.path.basename(filepath)\n","\t\tall_filenames.append(base)\n","\tfor filepath in tqdm(paths):\n","\t\tresult = model.transcribe(filepath)\n","\t\toutput = result[\"text\"].lstrip()\n","\t\toutput = output.replace(\"\\n\",\"\")\n","\t\tthefile = str(os.path.basename(filepath).lstrip(\".\")).rsplit(\".\")[0]\n","\t\toutfile.write(thefile + '|' + output + '|' + output + '\\n')\n","\t\tprint(thefile + '|' + output + '|' + output + '\\n')"],"metadata":{"id":"9LxJ0MF9rvOx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Check your metadata.csv file\n","!cat $open_path"],"metadata":{"id":"5fQ_3SuqDiaK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Prepare Training"],"metadata":{"id":"L9QBhAWvcPsk"}},{"cell_type":"markdown","source":["If there is an error about mecab-python 3, refer to this [github](https://github.com/SamuraiT/mecab-python3#common-issues)"],"metadata":{"id":"z-eKOq1MuHUd"}},{"cell_type":"code","source":["!tts --text \"I am the very model of a modern Major General\" --model_name \"tts_models/en/ljspeech/vits\" --out_path /content/ljspeech-vits.wav"],"metadata":{"id":"v2TAUuFo68EC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","%load_ext tensorboard"],"metadata":{"id":"fTGjP2H5cVKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls -al $output_path"],"metadata":{"id":"OGP8kP0ZdO9I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Set the tensorboard. Refresh the tensorboard to track the training process\n","%tensorboard --logdir $output_path"],"metadata":{"id":"9neGzHAFcgG9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from trainer import Trainer, TrainerArgs\n","from TTS.tts.configs.shared_configs import BaseDatasetConfig\n","from TTS.tts.configs.vits_config import VitsConfig\n","from TTS.tts.datasets import load_tts_samples\n","from TTS.tts.models.vits import Vits, VitsAudioConfig\n","from TTS.tts.utils.text.tokenizer import TTSTokenizer\n","from TTS.utils.audio import AudioProcessor"],"metadata":{"id":"tmg7XCDccn3d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SKIP_TRAIN_EPOCH = False"],"metadata":{"id":"ys2gUivWcw20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_config = BaseDatasetConfig(\n","    formatter=\"ljspeech\", meta_file_train=open_path, path=mono\n",")"],"metadata":{"id":"qOohsYTic1sg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We are using ***VITS*** model here! You can utilize other pre-trained models. Refer to official [Coqui-AI](https://tts.readthedocs.io/en/latest/models/vits.html)"],"metadata":{"id":"oZPlwxHJuy1M"}},{"cell_type":"code","source":["audio_config = VitsAudioConfig(\n","    sample_rate=22050, win_length=1024, hop_length=256, num_mels=80, mel_fmin=0, mel_fmax=None\n",")\n","\n","config = VitsConfig(\n","    audio=audio_config,\n","    run_name=\"vits_ljspeech\",\n","    batch_size=16,\n","    eval_batch_size=16,\n","    batch_group_size=16,\n","#    num_loader_workers=8,\n","    num_loader_workers=2,\n","    num_eval_loader_workers=2,\n","    run_eval=True,\n","    test_delay_epochs=-1,\n","    epochs=25000, #adjust the epoch\n","    save_step=20,\n","\t  save_checkpoints=True,\n","\t  save_n_checkpoints=4,\n","\t  save_best_after=1000,\n","    #text_cleaner=\"english_cleaners\",\n","    text_cleaner=\"multilingual_cleaners\",\n","    use_phonemes=False,\n","    phoneme_language=\"en-us\",\n","    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n","    compute_input_seq_cache=True,\n","    print_step=25,\n","    print_eval=True,\n","    mixed_precision=True,\n","    output_path=output_path,\n","    datasets=[dataset_config],\n","    cudnn_benchmark=False,\n",")\n","\n","# INITIALIZE THE AUDIO PROCESSOR\n","ap = AudioProcessor.init_from_config(config)\n"],"metadata":{"id":"9y06T6djc8AV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# INITIALIZE THE TOKENIZER\n","tokenizer, config = TTSTokenizer.init_from_config(config)\n","\n","# LOAD DATA SAMPLES\n","# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n","train_samples, eval_samples = load_tts_samples(\n","    dataset_config,\n","    eval_split=True,\n","    eval_split_max_size=config.eval_split_max_size,\n","    eval_split_size= config.eval_split_size, #Small number of samples can cause an error\n",")"],"metadata":{"id":"PLQiiKF6c_P4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Vits.init_from_config(config)"],"metadata":{"id":"aRLoWV3xdBNn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["run_type = \"restore\" #@param [\"continue\",\"restore\",\"restore-ckpt\"]\n","print(run_type + \" run selected\")"],"metadata":{"id":"Dt5kvB016irA","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["run_folder = \"Replace here to the checkpoint folder you want to continue on\" #@param {type:\"string\"}"],"metadata":{"id":"g-CLg7k_gUrN","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datetime\n","def get_today_yymmdd():\n","    today = datetime.datetime.now()\n","    return today.strftime(\"%y%m%d\")\n","\n","date = get_today_yymmdd()\n","\n","ckpt_file = \"checkpoint_\"+date+\"_\"+ds_name+\".pth\" #@param {type:\"string\"}\n","print(ckpt_file + \" selected for restore run\")\n","if run_type==\"continue\":\n","  print(\"Warning:\\n restore checkpoint selected, but run type set to continue.\\nTrainer will load best loss from checkpoint directory.\\n Are you sure this is what you want to do?\\n\\nIf not, change the run type below to 'restore'\")\n","elif run_type==\"restore-ckpt\":\n","  print(\"Warning:\\n restore checkpoint selected, run type set to restore from selected checkpoint, not default base model.\\nIf this is not correct, adjust the run type.\")\n"],"metadata":{"id":"VUzuD7rSeHVd","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["run_type = \"restore\" #@param [\"continue\",\"restore\",\"restore-ckpt\"]\n","print(run_type + \" run selected\")"],"metadata":{"id":"5muXWIjOe5wG","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(run_type)\n","if run_type==\"continue\":\n","  CONTINUE_PATH= output_path+run_folder\n","  trainer = Trainer(\n","    TrainerArgs(continue_path=CONTINUE_PATH, skip_train_epoch=SKIP_TRAIN_EPOCH),\n","    config,\n","    output_path=output_path,\n","    model=model,\n","    train_samples=train_samples,\n","    eval_samples=eval_samples,\n",")\n","elif run_type==\"restore\":\n","    trainer = Trainer(\n","    TrainerArgs(restore_path=MODEL_FILE, skip_train_epoch=SKIP_TRAIN_EPOCH),\n","    config,\n","    output_path=output_path,\n","    model=model,\n","    train_samples=train_samples,\n","    eval_samples=eval_samples,\n",")\n","elif run_type==\"restore-ckpt\":\n","  trainer = Trainer(\n","  TrainerArgs(restore_path=output_path + run_folder+\"/\"+ckpt_file, skip_train_epoch=SKIP_TRAIN_EPOCH),\n","  config,\n","  output_path=output_path,\n","  model=model,\n","  train_samples=train_samples,\n","  eval_samples=eval_samples,\n",")"],"metadata":{"id":"UkWYB-Hve7fD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LET'S START TRAINING!"],"metadata":{"id":"iRCNXX-Gpqzt"}},{"cell_type":"code","source":["trainer.fit()"],"metadata":{"id":"GyTw0q7ngmp1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Take a look at the results üîä"],"metadata":{"id":"U--7v5J0lvYq"}},{"cell_type":"code","source":["ckpts = sorted([f for f in glob.glob(output_path+\"/*/*.pth\")])\n","configs = sorted([f for f in glob.glob(output_path+\"/*/*.json\")])\n","save_file = test_path + \"test_audio.wav\"\n","\n","print(\"ckpts: \",ckpts[0])\n","print(\"configs: \",configs[0])\n","print(\"Saved file_name: \",save_file)"],"metadata":{"id":"vJlMFeVWI_dt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import locale\n","# locale.getpreferredencoding = lambda: \"UTF-8\""],"metadata":{"id":"2cudtiLt84uQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import subprocess\n","\n","text = \"Hello, nice to meet you.\"\n","\n","command = f\"tts --text \\\"{text}\\\" --model_path \\\"{ckpts[0]}\\\" --config_path \\\"{configs[0]}\\\" --out_path \\\"{save_file}\\\"\"\n","subprocess.run(command, shell=True)\n"],"metadata":{"id":"bHx9ZTLH90zo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import IPython\n","import librosa\n","\n","# Load the audio file and get the audio data and sampling rate\n","audio_data, sampling_rate = librosa.load(save_file, sr=None)\n","\n","# Display the audio using IPython.display.Audio\n","IPython.display.Audio(data=audio_data, rate=sampling_rate)\n"],"metadata":{"id":"9KrLZZgi-BZH"},"execution_count":null,"outputs":[]}]}